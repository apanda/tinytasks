\section{Architecting for Tiny Tasks}
\label{sec:architecture}
% Argue that we need a new architecture
While existing frameworks like Spark benefit from the use of smaller tasks, Figure~\ref{fig:sparkskew}
shows the need for a new task execution framework to truly support tiny tasks across a wide
variety of jobs.
Supporting tiny tasks requires a highly scalable scheduler that can handle hundreds 
of thousands of scheduling decisions per seconds. We also argue that efficiently using
tiny tasks requires task launch overheads be reduced by two orders of magnitude to around $100\mu$s.
Efficiently utilizing CPU and I/O resources with low task runtimes, requires
a pipelined task execution model where the execution framework manages I/O
requests for input, output and intermediate data. In this section
we discuss these components and other changes required to allow all jobs to use
tiny tasks, and discuss challenges associated with converting certain jobs.

\subsection{Architecture Overview}
We propose a tiny task execution model that supports computations expressed using 
a variety of popular programming frameworks (e.g., MapReduce~\cite{dean2008mapreduce},
Spark~\cite{zaharia2010spark}, DryadLINQ~\cite{yu2008dryadlinq}). 
A job is composed of a number of tasks, each representing an indivisible
and idempotent unit of execution. A task consists of a set of named inputs
and code that operates on these inputs, and runs on a single machine.
A tasks inputs are restricted to around 8MB. The task execution framework
pre-fetches inputs before running a task and is responsible for saving
outputs to disk or transferring them over the network.  We use a cooperative 
scheduling model, i.e., resources are reassigned after tasks explicitly release 
them on finishing. Task runtimes are limited by limiting input size and
appropriate modifications to programming frameworks. Section~\ref{sec:alternate} 
compares our design to other alternatives.

\subsection{Low-latency scheduling}
Supporting tiny tasks requires a low-latency, high throughput cluster scheduler.
For instance, supporting $100$ms tasks in a cluster with $160,000$ cores
(e.g., 10,000 16-core machines),
requires a scheduler that can, on average, make $1.6$ million scheduling
decisions per second.
Today's centralized schedulers have well-known scalability
limits~\cite{wilkesberkeley} that
hinder their ability to support tiny tasks in a large cluster.
Recently proposed distributed scheduling techniques including batch
sampling~\cite{ousterhoutbatch} have shown that distributed schedulers
can scale well and beyond millions decisions per second at near-optimality.
Our proposed system relies on the use of such distributed schedulers.

In addition to providing high throughput scheduling decisions, a framework for
tiny tasks must also reduce the latency for launching individual tasks,
especially in the case of cooperative scheduling.
Consider for an average task length of $100$ms, in order to achieve 99\% utilization,
task launch overhead should be less than $1$ms.
Popular frameworks like Hadoop MapReduce
have task launch overheads of many seconds, largely due to the use of
heartbeat messages to communicate scheduling decisions and the need to launch new
processes for each task.
Newer frameworks like Spark reduce task launch overhead to tens of
milliseconds using an event-driven architecture and maintaining
an active thread pool for execution~\cite{shark-tr}.
With the use of distributed schedulers and more careful engineering,
it is attainable to reduce the task launch overhead to a level much lower than the
required $1$ms, as it has been shown that $5\mu$s-$10\mu$s RPCs are possible
in the short term~\cite{low-latency}.

\subsection{Pipelined Execution}
\label{sec:pipeline}
To optimize for short tasks, we propose pipelining the phases of task execution.
Current task execution involves a task fetching data from the disk, or over the network,
processing this data, and storing the output either on memory or on disk. We observe
that these three steps are independent phases. We propose pipelining these phases such that one task's
input is fetched while another task is running, thus more fully utilizing both
the CPU and the I/O bus.

Pipelining is particularly powerful when combined with small tasks. With
today's large tasks, tasks accumulate large amount of output data in memory.
Often, a task's output data will overflow the available space in memory
and spill onto a disk, which is a well-known cause of poor MapReduce
performance~\cite{lipcon2012optimizing}. Tiny tasks fundamentally change the
resource footprint of tasks: since tasks run for a shorter period of time,
they generate less output data and thus use less memory. The framework can
explicitly control the remaining memory, choosing to cache the most important
and most frequently-accessed output data, and store the remaining data on disk
or on a different machine. Data written to disk can be written in the
background, while more tasks for the job are run, alleviating the performance
problems caused by spilling data to disk during task execution.
Shifting control of I/O entirely to the framework also allows for
powerful optimizations.  For a MapReduce-style job, for instance, the
framework could store the map outputs that will be used for the first set of
tiny reduce tasks in memory, and store the remaining outputs on disk. While
the first set of tiny reduce tasks are running, the framework can pipeline
reading data for the next set of tasks.  Giving frameworks better control over
I/O has been shown
to greatly improve network utilization and performance in
clusters~\cite{chowdhury2011managing, chowdhury2012coflow}.

\subsection{Scalable Storage Systems}
To allow tasks to complete quickly, we propose operating on data blocks of
at most $8$MB. Previous work has shown that $8$MB random disk reads achieve
approximately $88$\% of the bandwidth of sequential reads, and that smaller random
reads cause significant performance degradation due to disk
seeks~\cite{nightingale2012flat}. 

Using small data blocks requires a move away from traditional file systems like
HDFS, where scalability limitations prevent the use of small blocks.
Recent work on distributed filesystems, e.g., Flat Data Center
Storage (FDS)~\cite{nightingale2012flat}, address HDFS scalability concerns by
distributing metadata across multiple servers. While FDS addresses the scalability
concerns, effective use of tiny tasks requires close coordination between the file
system, the scheduler and the caching layer.
In particular, locality based scheduling, pipelined data access, caching blocks in 
memory, and effective coordination of network traffic are closely linked, and 
must be considered holistically. We propose building a globally aware, distributed
component which accounts for these factors, and provides additional information to
both the scheduler, and the filesystem.

\subsection{Programming model}
\label{sec:prog}
Most tasks in a data parallel framework can be split into tiny tasks by
reducing the input size; however, some types of tasks cannot easily be
parallelized.
%While splitting most tasks in a dataparallel framework by reducing input size
%is straight forward, splitting certain tasks might incur a higher overhead
%than the corresponding improvements.

Data parallel frameworks like Dryad~\cite{yu2008dryadlinq} use aggregation
trees to solve similar problems by progressively combining outputs from
multiple tasks. Leveraging aggregation trees will allow a tiny tasks framework
to reduce task lengths for a wider variety of tasks. For instance, consider a
job that groups values by some criterion, and calculates the mean for each group.
In this case, aggregation trees provide a mechanism for dividing an individual group
across multiple tasks.

Despite the use of aggregation trees, some tasks may remain difficult to
divide into tiny tasks. For example, tasks that are not associative and
commutative do not lend themselves to the use of aggregation trees. In our
example above, a more complicated operation might not lend itself to being
simplified using an aggregation tree.
To allow splitting these tasks, we propose providing a framework-maintained
temporary state store that can be used to communicate and share data between a
job's tiny tasks. Such a store could be used to implement a job that computes
distinct values in a file, for instance, by storing hashes of all values
seen so far. We envision that this store would provide a key-value interface,
and would provide strong consistency guarantees.

Inevitably, some tasks will be impractical to split, despite these tools.
To accommodate these
tasks, we plan on allowing some large tasks to run on the cluster. These tasks
may need to run on a small part of the cluster reserved for large tasks.
Alternately, we expect that if a small percentage of tasks remain large, they
can be run on the same infrastructure as tiny tasks without impacting the
performance of remaining tasks. Exploring the effects of such sharing is
the subject of ongoing research.
