\section{Architecting for Tiny Tasks}
\label{sec:architecture}

% Argue that we need a new architecture
While existing frameworks can benefit from the use of smaller tasks (as shown
in Figure~\ref{fig:sparkskew}), supporting tiny tasks for all jobs in a large cluster
requires addressing numerous challenges.
A cluster supporting tiny tasks must provide low task launch overheads and use
a highly scalable scheduler that can
handle hundreds of thousands of scheduling decisions per second. % and
%launch tasks with low overhead.
Tiny tasks operate on
small blocks of data, and hence require a scalable file system. To ensure that
tasks can complete quickly, we propose giving the framework more control
over I/O. % so that, for example, a task's data is already in-memory when the
%task is launched.
Finally, ensuring
that all jobs can be broken into tiny tasks requires some improvements
to the programming model; e.g., support for aggregation trees.
In this section, we discuss a preliminary architecture to address
each of these challenges and the lower bound that each challenge places
on response time; we aim for tasks that are as small as possible.
We find that tasks that complete in hundreds of milliseconds
are practical in the short-term, and that we can drive task launch overhead
down to further reduce task runtime in the future.

\subsection{Execution model}
We propose a task execution model that supports data-parallel computations
expressed using a variety of popular programming frameworks (e.g.,
MapReduce~\cite{dean2004mapreduce}, Spark~\cite{zaharia2010spark},
DryadLINQ~\cite{yu2008dryadlinq}).
A job is composed of a number of tasks, each representing an atomic
and idempotent unit of execution. A task consists of a set of named inputs
and code that operates on these inputs, and each task runs on a single machine.
We assume a cooperative scheduling model; i.e., tasks explicitly release
resources on completion (in contrast to preemptive models, discussed in
\S\ref{sec:preemption}).

%We assume that the framework is used to support data-parallel programs whose input
%data reside on disk, and the workload is I/O bound.

%To allow tasks to complete quickly, we propose operating on data blocks of
%at most $8$MB.
%Previous work has shown that $8$MB random disk reads achieve
%approximately $88$\% of the bandwidth of sequential reads, and smaller reads would
%cause significant performance degradation due to disk seeks~\cite{nightingale2012flat}. 
%Assuming $100$MB/s disk throughput in a fully pipelined execution framework, each task
%can execute up roughly $100$ms to process the $8$MB input.

%Although frameworks such as Spark are significantly better
%than traditional frameworks such as Hadoop in handling smaller tasks,
%Figure~\ref{fig:sparkskew} shows the need for a new execution framework
%to truly support tiny tasks at $100$ms range.

% While newer data-parallel frameworks like Spark can support tasks of finer-granularity,
% Figure~\ref{fig:sparkskew} shows that even in newer frameworks like Spark, 
% benefit from the use of
% smaller tasks, Figure~\ref{fig:sparkskew}
% shows the need for a new task execution framework to truly support tiny tasks across a wide
% variety of jobs.

%Supporting tiny tasks requires a highly scalable scheduler that can handle hundreds 
%of thousands of scheduling decisions per second. We also argue that efficiently using
%tiny tasks requires task launch overheads be reduced to $1$ms or less.
%Efficiently utilizing CPU and I/O resources with low task runtimes requires
%a pipelined task execution model where the execution framework manages I/O
%requests for input, output and intermediate data. We use a cooperative 
%scheduling model, i.e., resources are reassigned after tasks explicitly release 
%them on finishing. Task runtimes are limited by limiting input size and
%appropriate modifications to programming frameworks. Section~\ref{sec:alternate} 
%compares our design to other alternatives.


% The task execution framework
% pre-fetches inputs before running a task and is responsible for saving
% outputs to disk or transferring them over the network.  

\subsection{Scalable storage systems}

In the short term, we expect the time needed to read input data to be the
limiting factor in driving down task durations.
Previous work has shown that 8MB random disk reads can achieve approximately 88\%
of the throughput of sequential reads, and that smaller
random reads cause significant performance degradation due to disk seeks~\cite{nightingale2012flat}. Thus, as long as input data is stored on disk, we
require that tasks read at least 8MB of input data. Data-parallel
workloads are commonly I/O bound, so we expect task runtime to be dominated
by time taken to read input data; thus,
assuming 100MB/s disk throughput, 8MB input data sizes should result in task durations
of hundreds of milliseconds.

Using small data blocks requires a move away from traditional distributed file systems like
HDFS, where scalability limitations prevent the use of small blocks.
While HDFS does allow reading only part of a block, having multiple tiny tasks
that operate on the same file block limits parallelism.
Recent work on distributed filesystems, e.g., Flat Data Center
Storage (FDS)~\cite{nightingale2012flat}, addresses these scalability concerns by
distributing metadata across multiple servers. 

The effective use of tiny tasks also requires close coordination between the file
system, the scheduler, and the caching layer.
In particular, locality based scheduling, pipelined data access, caching blocks in 
memory, and effective coordination of network traffic are closely linked, and 
must be considered holistically. We propose building a globally aware, distributed
component that accounts for these factors.
%, and provides additional information to
%both the scheduler, and the filesystem.



\subsection{Low-latency scheduling}
Supporting tiny tasks requires a low-latency, high throughput cluster scheduler.
Handling $100$ms tasks in a cluster with $160,000$ cores
(e.g., 10,000 16-core machines),
requires a scheduler that can, on average, make $1.6$ million scheduling
decisions per second.
Today's centralized schedulers have well-known scalability
limits~\cite{wilkes2013omega} that
hinder their ability to support tiny tasks in a large cluster.
Engineering improvements such as compressing task descriptions,
avoiding sending the same task description to the same machine multiple
times, and using more efficient networking have helped some
centralized schedulers
provide higher throughput~\cite{zaharia2012meetup}.
However, handling large clusters and very
short tasks may require a decentralized scheduler design.
Recently proposed distributed scheduling techniques including batch
sampling~\cite{ousterhoutbatch} have shown that distributed schedulers
can scale well beyond millions of decisions per second at near-optimality.
Our proposed system relies on the use of such distributed schedulers.
%Since a job completes only when all of its tasks complete, we also plan to 
%explore techniques like gang scheduling~\cite{feitelson1992gang} to minimize
%overall job completion time.

In addition to providing high throughput scheduling decisions, a framework for
tiny tasks must also reduce the overhead for launching individual tasks.
%especially in the case of cooperative scheduling.
%Consider for an average task length of $100$ms, in order to achieve 99\% utilization,
%task launch overhead should be less than $1$ms.
%Popular frameworks like Hadoop MapReduce
%have task launch overheads of many seconds, largely due to the use of
%heartbeat messages to communicate scheduling decisions and the need to launch new
%processes for each task.
Popular frameworks like Hadoop MapReduce have task launch overheads of many
seconds, due to a variety of factors including the need to launch a new
JVM for each task; newer frameworks like Spark~\cite{zaharia2010spark} reduce
overhead to $5$ms.
To support tasks that complete in hundreds of milliseconds, we argue for
reducing task launch overhead even further to $1$ms so that launch overhead
constitutes at most 1\% of task runtime.
By maintaining an active threadpool for task execution on each worker node
and caching binaries, task launch overhead can be reduced to the time to
make
 a remote procedure
call to the slave machine to launch the task. Today's datacenter networks
easily allow a RPC to complete within $1$ms. In fact, recent work showed that
$10\mu$s RPCs are possible in the short term~\cite{low-latency}; thus,
with careful engineering, we believe task launch overheads of $50\mu$s are
attainable. $50\mu$s task launch overheads would enable even smaller tasks
that could read data from in-memory or from flash storage in order to complete
in milliseconds.
\eat{
so that launch overhead constitutes
less than 1\% of task runtime.
A framework for tiny tasks 
If the framework maintains an active threadpool for task execution on each worker,
task launching should be essentially a remote procedure call.
With careful engineering,
it is attainable to reduce the overhead to a level much lower than the
required $1$ms, as it has been shown that $5\mu$s-$10\mu$s RPCs are possible
in the short term~\cite{low-latency}.
}
%\subsection{Pipelined Execution}
\subsection{Framework-controlled I/O}
\label{sec:pipeline}
%To optimize for short tasks, we propose pipelining the phases of task execution.
%Current task execution involves a task fetching data from the disk, or over the network,
%processing this data, and storing the output either on memory or on disk. We observe
%that these three steps are independent phases. We propose pipelining these phases such that one task's
%input is fetched while another task is running, thus more fully utilizing both
%the CPU and the I/O bus.

Using tiny tasks requires providing the framework with additional control to optimize I/O.
%Pipelining is particularly powerful when combined with small tasks.
Today's large tasks accumulate a large amount of output data in memory.
Often this output data will exceed available memory, 
and will spill onto a disk, leading to poor MapReduce
performance~\cite{lipcon2012optimizing}. Tiny tasks fundamentally change the
resource footprint of tasks: since tasks run for a shorter period of time,
they generate less output data and thus use less memory. The framework can
explicitly control the remaining memory, choosing to cache the most important
and most frequently-accessed output data, and store the remaining data on disk
or on a different machine. Data written to disk can be written in the
background, while more tasks for the job are run, alleviating the performance
problems caused by spilling data to disk during task execution.
Shifting control of I/O entirely to the framework also allows for
powerful optimizations.  For a MapReduce-style job, for instance, the
framework could store the map outputs that will be used for the first set of
tiny reduce tasks in memory, and store the remaining outputs on disk. While
the first set of tiny reduce tasks are running, the framework can pipeline
reading data for the next set of tasks.  Giving frameworks better control over
I/O has been shown
to greatly improve network utilization and performance in
clusters~\cite{chowdhury2011managing, chowdhury2012coflow}.

\subsection{Programming model}
\label{sec:prog}
Most tasks in a data parallel framework can be split into tiny tasks by
reducing the input size; however, some types of tasks cannot easily be
parallelized. Parallelizing all jobs is the most significant challenge
in realizing tiny tasks. 
Consider, for example, reduce tasks in a MapReduce job. In the limit,
one reduce task can be launched to handle each key. However, if all
values map to the same key, that key cannot easily be split into multiple
tasks.
%granularity of a reduce task
%is to process a single key.
%When a single key contains a large number of values, the reduce task might
%violate the tiny task constraint.
Data parallel frameworks like Dryad~\cite{yu2008dryadlinq} use aggregation trees to solve
similar problems by progressively combining inputs from multiple tasks;
aggregation trees will allow parallelizing tasks where the function
is commutative and associative.

%While splitting most tasks in a dataparallel framework by reducing input size
%is straight forward, splitting certain tasks might incur a higher overhead
%than the corresponding improvements.

% Some data parallel frameworks like Dryad~\cite{yu2008dryadlinq} use aggregation
% trees to solve similar problems by progressively combining outputs from
% multiple tasks. Leveraging aggregation trees will allow a tiny tasks framework
% to reduce task lengths for a wider variety of tasks. For instance, consider a
% job that groups values by some criterion, and calculates the mean for each group.
% In this case, aggregation trees provide a mechanism for dividing an individual group
% across multiple tasks.

Despite the use of aggregation trees, some tasks may remain
difficult to divide into tiny tasks. 
%For example, tasks that are not associative and
%commutative do not lend themselves to the use of aggregation trees.
%In our example above, a more complicated operation might not lend itself to being
%simplified using an aggregation tree.
To split these tasks, we propose providing a framework-maintained
temporary state store that can be used to communicate and share data between a
job's tiny tasks. Such a store could be used to implement a job that computes
distinct values in a file, for instance, by storing hashes of all values
seen so far. We envision that this store would provide a key-value interface,
and would provide strong consistency guarantees.

Inevitably, some tasks will be impractical to split, despite these tools.
To accommodate such
tasks, we plan on allowing some large tasks to run on the cluster. We expect
that if a small percentage of tasks remain large, they can be run on the same 
infrastructure as tiny tasks without impacting the performance of remaining tasks.
Exploring the impact of such sharing is the subject of ongoing research.
%These tasks
%may need to run on a small part of the cluster reserved for large tasks.
%Alternately, we expect that if a small percentage of tasks remain large, they
%can be run on the same infrastructure as tiny tasks without impacting the
%performance of remaining tasks. Exploring the effects of such sharing is
%the subject of ongoing research.
