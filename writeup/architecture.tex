\section{Architecting for Tiny Tasks}
% TODO(josh): maybe the bit about "different kinds" can be made clearer?
In this section we explore architectural challenges in designing a cluster
computing framework designed for tiny-task. Such a framework must support
low-overhead, high-throughput scheduling, scalable filesystems, and provide
a programming model that can support a wide variety of job.

\subsection{Execution Model}
Our proposed model relies on an execution model where every job is comprised of
a large number of tiny tasks. Tiny tasks, similar to existing tasks, represent an
indivisible, and idempotent unit of execution. A task is comprised of a set of named
inputs, and code that operates on these inputs. In our
model, inputs for a task are small, usually $8$MB or smaller, and tasks execute in
around $100$ ms. Our execution model also requires that the framework, rather than the
task be responsible for fetching inputs from disk, and writing outputs to disk, or transmitting
them over the network. This allows the framework to prefetch inputs for tasks, and
to optimize whether outputs are written to disk or kept in memory. We envision using
a cooperative scheduling model, i.e., tasks must explicitly hand back resources (by finishing)
before these resources are reassigned. Task runtimes are limited by limiting input size.
Section~\ref{sec:alternate} compares our design to other alternates.

\subsection{Low-latency scheduling}
Reducing task runtimes requires that cluster schedulers make scheduling decisions more frequently.
For instance in a cluster with a $1000$ slots, task lengths of $100$ ms requires a scheduler make
$10^6$ scheduling decisions a second on average. Centralized scheduling schemes have well known
scalability limits~\cite{wilkesberkeley}, which a framework using tiny tasks must address. Recently
proposed distributed scheduling techniques like batch sampling~\cite{ousterhoutbatch} can provide
near optimal scheduling, while also exhibiting better scalability. Our proposed system relies
on the use such distributed schedulers, and extensions supporting fairness guarantees, utilization
guarantees, and other such properties. To reduce additional task latency due to data movement, our
proposed scheduler will also support task constraints, and it has already been demonstrated that
these constraints can be enforced by techniques like batch sampling. 

In addition to scheduling latency, a framework using tiny tasks must also reduce the overhead
for an individual tasks. Existing frameworks have a task launch overhead ranging from a few
seconds for Hadoop, to a few hundred milliseconds for Spark. Studies of network round trip times
in datacenters, and the overhead for launching a task using a threadpool in existing operating
 systems lead us to believe that this overhead can be reduced to approximately $100 \mu$s, making
it insignificant in comparison to the overall task runtime.

\subsection{Pipelined Execution}
\label{sec:pipeline}
Current task execution generally happens in three phases, a fetch phase where a tasks
inputs are fetched from a file, or over the network, a compute phase where the task is actually
executed, and an output phase where the outputs are stored in memory, disk, or transmitted over
the network. We propose pipelining these phases, with one tasks inputs being fetched while another
is running, thus more fully utilizing both the CPU, and the I/O bus. In addition to increasing such
utilization, such pipelining allows the cluster framework to better schedule both disk and network
I/O to maximize throughput, and reduce contention. It has previously been shown~\cite{chowdhury2011managing,
chowdhury2012coflow} to greatly improve network utilization, and performance in clusters.

\subsection{Scalable Storage Systems}
We limit task runtime by limiting the size of a tasks input. Even with pipelining, we
expect a tasks inputs to be readable within the runtime of a single task. Current disks exhibit
throughputs of around $100$MB/s, and thus a single tasks inputs can be no more than $10$MB. Based
on these calculations, and results from FDS~\cite{nightingale2012flat} we require using data blocks
that are no more than $8$MB. Reducing blocks to this size negatively impacts the performance for
existing distributed filesystems like HDFS which use a centralized metadata server. Recent work on
distribute filesystems like FDS address these concerns by distributing metadata cross multiple
servers. While FDS provides the first steps towards a filesystem for tiny tasks, we envision
one would need to make changes to more fully integrate this system with the scheduler, and
the pipelining component.

\subsection{Programming model}
\label{sec:prog}
While splitting most tasks in a dataparallel framework by reducing input size 
is relatively straight forward, splitting certain tasks might incur a higher overhead 
than the corresponding improvements. For instance, consider an all-to-all shuffle, involves
communication across $O(N^2)$ pairs of senders and receivers. The increased communication cost
for such tasks might not be offset by the additional degree of parallelism. Aggregation trees are
commonly used in data parallel frameworks like Dryad~\cite{yu2008dryadlinq} to solve similar
problems by progressively combining outputs from multiple tasks. We plan on using such trees,
and extensions to these techniques to provide tools that can allow frameworks to reduce task
lengths for arbitrary tasks.

There might however still remain tasks, which are either not associative and commutative, or
require temporary state, which might not be easily divided up. We propose providing a 
framework maintained temporary store that can be used by tasks to communicate, and reuse data
across invocations. Such a store could for instance be used to implement a task computing distinct
values for a file. We envision that such a store would provide an interface similar to key-value
stores, and would provide strong consistency guarantees.

Finally there are those tasks which cannot be split despite these tools. To accommodate these 
tasks we plan on allowing some large tasks to run on the cluster. To provide the guarantees offered
by tiny tasks, such tasks would be required to run on a small part of the cluster reserved for large
tasks. Larger reservations allow such tasks to finish quicker, while potentially reducing cluster 
utilization. We envision providing the cluster administrator with controls to change the precise mixture.
Exploring the effects of such sharing, and the tradeoff space offered by such control is subject of ongoing research.
