\section{Discussion}

\subsection{Alternate Design Schemes}
\label{sec:alternate}
Our choice of a cooperative multi-tasking scheme is in contrast to
preemption based schemes commonly used in operating systems. The benefits of 
preemption and short quanta are well studied in operating
systems literature; for instance~\cite{sherman1972trace} shows the benefits
of short scheduling quanta. We however find that preemptive mechanisms are
ill-suited to data parallel jobs running on a cluster as discussed below.\\
%However achieveing low overhead task-switching is more expensive in the distributed setting. 
\textbf{Cost of task-switching}: Work on process
migration~\cite{douglis1991transparent,milojivcic2000process} and virtual
machine migration~\cite{clark2005live} has shown mechanisms to transparently
move tasks across machines. Such methods however involve a high overhead, as
migrating a task involves transferring both task context, a tasks intermediate
data, and its inputs. For data parallel applications, input data, and intermediate
data can be several gigabytes, incurring very high overheads.

Cooperative multitasking by contrast eliminates the need to migrate such data. In
addition we envision moving shared state into a framework maintained scratch space,
by explicitly differentiating such state, we envision it can be more easily migrated.

\textbf{Fault tolerance:} In a datacenter setting, tiny tasks are better suited for
fault tolerance when compared to task preemption. Since each tiny task is a
deterministic unit of work, tasks can be executed in parallel during recovery.
In contrast, using preemption one would need to maintain redundant checkpoints
and fault recovery cannot be executed in parallel. \\

\textbf{Enforcing tiny tasks:} Preemption allows a framework to finely control
task lengths. However controlling task inputs, and minor modifications to the 
framework should allow us to effectively control task lengths.\\

\subsection{Skew-Handling in Clusters}
A separate line of research has focused on skew-mitigation to improve job
performance in datacenters. Examples of such work include
Mantri~\cite{ananthanarayanan2010reining}, SkewTune~\cite{kwon2012skewtune},
Scarlett~\cite{ananthanarayanan2011scarlett}, and the work on task
speculation~\cite{zaharia2008improving}. Mantri, and Scarlett, attempt to
mitigate task runtime skew by modeling the causes for skew, and accounting for
these. In particular Mantri performs resource aware scheduling to decrease the
probability of observing task skews, while Scarlett replicates storage blocks
based on their probability to decrease the wait time for a popular block. While
both of these systems moderately reduce task skew, they rely on a fragile set of
signals, and do not work in all cases. Using tiny tasks naturally overcomes data
skew among reduce tasks as fine-grained hash partitioning ensures that the data is
more evenly spread across tasks. As shown in 
Section~\ref{sec:benefits}, using tiny tasks also allows work to be balanced across
different machines thereby overcoming skew due to slow machines.
Furthermore, existing skew mitigation techniques trade-off cluster resources to
gain more predictable task runtimes. This limits the applicability of these
techniques under situations of high load, where such guarantees might be most
important.

