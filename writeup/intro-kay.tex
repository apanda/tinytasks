\section{Introduction}
Computer systems schedule and manage a variety of tasks:
%Today's systems accept work in discrete units:
networks process flows of data
between two endpoints, operating systems execute individual applications, and
data centers process data in individual jobs.  In the context of networks and
operating systems, system designers have found that large, indivisible units of work limit utilization and load balancing,
and complicate fair sharing.  Instead, networks divide large flows into small
packets, and operating systems run applications in pre-emptable units of a
few milliseconds.  This paper argues for
applying a similar model to data centers by breaking jobs into
a large number of ``tiny tasks'', each of which runs for a few hundred
milliseconds.

Tiny tasks solve two major problems in today's datacenters:
%Decreased task runtimes solve two major problems in today's datacenters:

\vspace{4pt}\noindent\textbf{Batch and interactive sharing:}
Current clusters are forced to tradeoff utilization and responsiveness:
if a cluster is highly utilized and a high priority job arrives, the
high priority job may
need to wait for long-running tasks to complete before it can be launched.
Tiny tasks allow a cluster to be \emph{both} highly utilized and highly
responsive, since small tasks ensure frequent opportunities for new,
higher priority jobs to be launched.
\eat{
Long task runtimes make it challenging to run both batch and interactive
jobs on the same cluster. In particular, task lengths place a lower bound on
the time before resources allocated to a running task can be recovered. A new
task therefore must wait for running tasks to finish before it can start running.
This waiting time is a significant contributor to task latency, and thus adversly
affects the request latency for interactive jobs.

One must tradeoff between
cluster utilization and responsiveness, limiting the benefits of sharing
a cluster. By reducing task runtimes to an acceptably small value, tiny
tasks allow batch and interactive jobs share the same resources, without
trading off request latency.
}

\vspace{4pt}\noindent\textbf{Straggler mitigation:}
Prior work has shown that job runtimes are largely determined by
stragglers: tasks that take much longer to complete than other tasks in the
job. This problem has led to a wide variety of techniques to ensure that
work is evenly balanced across tasks, and to speculatively launch
redundant tasks when a task appears to be running on a slow machine.
Tiny tasks eliminate the straggler problem because work is allocated to
machines at fine granularity, so work will automatically be evenly spread
over available resources, and slower machines will automatically be assigned
less work.
\eat{
Prior work has shown that tasks runtimes exhibit a long
tailed distribution, and are highly variable. This variance can be caused by
a number of factors, including slow machines, congested networks, a single task
processing larger amount of data, or a combination of the above.
Many mechanisms have been suggested for mitigating
the effects of this variability, generally either by avoiding causes of these
long task runtimes, or by speculatively launching additional tasks in response to slow tasks.
By providing the scheduler with fine grained control over job execution, tiny
tasks makes such mitigation easier, allowing the scheduler to change the resources
allocated to a job in response to outliers.\\
}

Existing data-parallel systems have engineering limitations in their distributed
file systems and cluster schedulers that prevent short task runtimes. For example, in
large Hadoop clusters, a larger block size is advised to avoid overflowing the name node.
Hadoop also piggybacks the heartbeat messages to send scheduling decisions, which
results in high scheduling latency.
Current frameworks need to be re-architected to enable tasks to be broken into even smaller
units.
Recent work on distributed filesystems\cite{nightingale2012flat} and cluster
schedulers\cite{ousterhoutbatch} present the first steps towards
building a cluster framework that allows for tiny tasks. While these new filesystems
and schedulers address the scalability problems, many challenges remain. In particular,
efficient use of tiny tasks requires that the overhead for launching a task is small. Current
frameworks take on the order of several hundred milliseconds to a second to launch a task, negating
many of the gains provided by a system with sub-second task lengths. Similarly, any system
supporting tiny tasks must provide additional architectural support for more easily dividing tasks.

\begin{figure*}[!ht]
\centering
\subfigure[] {\includegraphics[width=0.45\textwidth]{figures/binpacked1-sep}
\vspace{-1.5in}
\label{fig:binpacked}
}
\hspace{0.2in}
\subfigure[] {
\includegraphics[width=0.45\textwidth]{figures/spark-skew-results}
\vspace{-1.5in}
\label{fig:sparkskew}
}
\caption{\subref{fig:binpacked} Improvement from perfectly balancing the total machine time for the
job across tasks. \subref{fig:sparkskew} Improvement from using tiny tasks in a cluster where 20\% of machines
take 21x longer to run each task. Error bars depict standard deviation.}
\vspace{-0.1in}
\end{figure*}

In this paper we present arguments and results motivating a move towards tiny tasks,
and we present preliminary design for a system supporting tiny tasks. In particular,
we propose a system that supports $100$ microsecond task launches, and an architecture
that allows most general applications to be expressed in terms of a set of tiny tasks.

\eat{We begin by quantifying the benefits of tiny tasks, using a series of simulations,
and application built on Spark\cite{zaharia2010spark} demonstrating the potential benefits
of using Spark. In Section~\ref{something} we outline the architecture for our system supporting tiny tasks, and
try and determine an appropriate task length based on task launch overheads, and other factors.
Next in Section~\ref{somethingelse} we discuss how to convert arbitrary jobs to better take advantage of tiny tasks, following
hich we discuss related work in Section~\ref{something}. Finally we conclude in Section~\ref{conclusion}.}
