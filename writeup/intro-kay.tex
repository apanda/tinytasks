\section{Introduction}
Today's systems accept work in discrete units: networks process flows of data
between two endpoints, operating systems execute individual applications, and
data centers process data in individual jobs.  In the context of networks and
operating systems, system designers have found that indivisible units of work are
inconvenient: large, indivisible units of work limit utilization and load balancing,
and complicate fair sharing.  Instead, networks divide large flows into small
packets, and operating systems run applications in pre-emptable units of a 
few milliseconds.  This paper argues for
bringing elasticity to the data center by breaking jobs up into
millions of ``tiny tasks'' that each complete in hundreds of milliseconds.

By inreasing elasticity, tiny tasks solve two major problems in today's datacenters:
\begin{itemize}
\item \textbf{Running batch jobs alongside interactive ones}: Running interactive
jobs is difficult because long-running tasks for batch jobs may be
using the resources needed by an interactive job (mention Amoeba tradeoffs
here?). Bursts of interactive jobs
cannot be serviced quickly, because they must wait for batch jobs to complete.
This problem frustrates data analysts trying to run real-time queries, and makes
running user-facing services alongside batch services impossible. Breaking large
jobs into millions of tiny tasks, each with runtime similar to the runtime for a
task in a user-facing service, eliminates this problem. We have found that
tiny tasks reduce the wait time for short tasks by a factor of X.
\item \textbf{Skew}: Skew affects tasks in a number of ways: a single task
may be processing more data than other tasks in the job, it may be processing
data that is more computationally complex, or a slow machine may cause tasks
to take longer for arbitrary reasons (stagglers). Tiny tasks avoid the need
for complex skew mitigation techniques by turning a data partitioning problem
    into a scheduling problem.  Rather than needing to evenly partition data
    in advance, with tiny tasks, a scheduler simply needs to assign tasks to
    machines as resoures become available, thus automatically balancing a job's
    work over the available resources. By architecting for skew, tiny tasks
    reduce average job completion time by a factor of X.
\end{itemize}

While tiny tasks can be used as a design principle in today's frameworks,
scalability limits of the scheduler and file system limit the size of a
cluster running tiny tasks; furthermore, some tasks cannot easily be
parallelized in today's frameworks. 
%Breaking jobs into tiny units of work requires a re-architecting of today's frameworks, which have high overheads that render sub-second tasks impractical.
We propose a system designed for tiny tasks that builds on recent research
that solves the above problems:
%We argue that these challenges are not insurmountable, and in fact, many
%have been solved by prior work:
\begin{itemize}
\item Task launch overhead: start JVM for MapReduce. Spark doesn't do this.
\item Scheduling: sparrow
\item Small file system blocks: FDS
\end{itemize}

This paper begins by quantifying the benefits of tiny tasks,
using Spark~\cite{??} to demonstrate that tiny tasks can improve performance
even in an unmodified framework (\S\ref{??}).
Next, we outline a system architected for tiny tasks, and explore how small
we can make tasks before the expected task launch overhead exceeds the benefit.
While smaller tasks offer many benefits, certain kinds of tasks are indivisible
in today's frameworks, or have performance that decreases sublinearly with
increased parallelimsm. In \S\ref{??}, we discuss how to convert these jobs
into tiny tasks using a new framework that provides distributed ``scratch
space.''
We conclude with a discussion of related work (\S\ref{??}).

\begin{comment}
Today's datacenters run increasingly diverse workloads. A decade ago, compute
clusters were designed for batch workloads: a typical job took hours to complete.
As users migrated large amounts of data to these clusters, they demanded ever
faster access to this data, spurring low-latency frameworks (e.g., Dremel, Spark,
Shark, Impala) that stripe work across thousands of machines or store data in
memory in order to complete jobs in seconds. While data analysts and user-facing
services rely on these low-latency frameworks, clusters continue to run
long-running, batch jobs (e.g., indexing the web), leading to a diverse mix of
job runtimes.  Workload studies from Facebook, Microsoft, Yahoo!, and Google
corroborate this claim, finding that job completion times range from seconds to days.
\end{comment}
