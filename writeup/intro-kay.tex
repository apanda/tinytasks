\section{Introduction}
Today's systems accept work in discrete units: networks process flows of data
between two endpoints, operating systems execute individual applications, and
data centers process data in individual jobs.  In the context of networks and
operating systems, system designers have found that large, indivisible units of work are
inconvenient: large, indivisible units of work limit utilization and load balancing,
and complicate fair sharing.  Instead, networks divide large flows into small
packets, and operating systems run applications in pre-emptable units of a 
few milliseconds.  This paper argues for
applying a similar modely to data centers by breaking jobs into
several ``tiny tasks'', each of which runs for a few hundred milliseconds.

Decreased task runtimes solve two major problems in today's datacenters:
\begin{itemize}
\item \textbf{Sharing a cluster for both batch and interactive workloads}: 
Long task runtimes make it challenging to run both batch and interactive
jobs on the same cluster. In particular, task lengths place a lower bound on
the time before resources allocated to a running task can be recovered. A new
task therefore must wait for running tasks to finish, before they can start running.
This waiting time is a significant contributor to task latency, and thus adversly
affects the request latency for interactive jobs.

One must tradeoff between
cluster utilization and responsiveness, limiting the benefits of sharing
a cluster. By reducing task runtimes to an acceptably small value, tiny
tasks allow batch and interactive jobs share the same resources, without
trading off request latency.

\item \textbf{Outliers}: Prior work has shown that tasks runtimes exhibit a long
tailed distribution, and are highly variable. This variance can be caused by
a number of factors, including a single task processing a larger amoung of data
than others, machines processing data at different reasons, network effects, or
a combination of such affects. Many mechanisms have been suggested for mitigating
the effects of this variability, generally either by avoiding causes of these
long task runtimes, or by launching additional tasks in response to slow tasks.
By providing the scheduler with fine grained control over job execution, tiny
tasks makes such mitigation easier, allowing the scheduler to change the resources
allocated to a job in response to outliers.
\end{itemize}

Scalability limitations of current centralized distributed file systems and cluster
scheduler limit the size of the cluster for which task lengths can be reduced. Similarly
current frameworks need to be rearchitected to enable tasks to be broken into even smaller
units. 
Recent work on distributed filesystems\cite{nightingale2012flat} and cluster schedulers\cite{sparrow}
present the first steps towards
building a cluster framework that allows for tiny tasks. While these new filesystems
and scheduler solve many of the scalability problems, many challenges remain. In particular
efficient use of tiny tasks requires that the overhead for launching a task is small. Current
frameworks take on the order of several of hundred milliseconds to a second to launch a task, negating
many of the gains providied by a system with sub-second task lengths. Similarly, any system
supporting tiny tasks must provide additional architecutural support for more easily dividing tasks.

In this paper we present the design for a system supporting tiny tasks, in particular
we propose a system that supports $100$ microsecond task launches, and an architecture
that allows most general application to be expressed in terms of a set of tiny tasks.

We begin by quantifying the benefits of tiny tasks, using a series of simulations, 
and application built on Spark\cite{zaharia2010spark} demonstrating the potential benefits
of using Spark. In Section~\ref{something} we outline the architecture for our system supporting tiny tasks, and 
try and determine an appropriate task length based on task launch overheads, and other factors.
Next in Section~\ref{somethingelse} we discuss how to convert arbitrary jobs to better take advantage of tiny tasks, following
hich we discuss related work in Section~\ref{something}. Finally we conclude in Section~\ref{conclusion}. 
