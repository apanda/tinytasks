\section{Related Work}

There has been a large body of work in the context of distributed operating 
systems~\cite{douglis1991transparent, milojivcic2000process,tanenbaum1990experiences, 
rozier1991overview} 
and virtual machines~\cite{tanenbaum1990experiences} that have looked at scheduling and
context-switching among processes across different machines. Migration of
processes or virtual machines typically use preemptive methods and we compare
them to our design goals in Section~\ref{sec:alternate}.

Recent work proposed Amoeba~\cite{ananthanarayanan2012true}, a system that
shares similar goals as our work. In Amoeba, the system identifies safe-points
where a task can be killed, while minimizing wasted work. At safe-points a task
can checkpoint output based on inputs it has processed so far, and a new task is
spawned to process the remaining inputs. This work is perhaps closest to
TinyTasks, and shows that preemptibility is an important feature for tasks
scheduled on a cluster.  However, while the work acknowledges the necessity for
preemptibility, it places no requirements on how frequently safe-point occur,
and hence places no bounds on the maximum time for which a single task can
capture the processor.  Furthermore, actually determining safe-points for
general tasks is non-trivial, and while Amoeba provides some examples of
safe-points for certain kinds of tasks, they do not provide a generic framework
for determining where such safe-points could occur.  TinyTasks by contrast do
not require any program analysis to identify such points, and instead relies on
limiting input sizes to achieve preemption.

Dremel~\cite{melnik2010dremel}, in conjunction with Google's cluster scheduler
Omega~\cite{wilkesberkeley}, has also been used to run interactive jobs in a
cluster otherwise used for batch processing. Dremel achieves low latency by
spinning up long-running agents on which queries are executed. This is similar
to statically partitioning a cluster, where a part of the cluster is reserved
for interactive jobs, and the rest can be used for batch jobs. Such a method
necessarily limits the utilization of a cluster, and requires that the peak load
for interactive jobs be known in advance, so as to ensure that constraints are
met. Requiring application requirements be known in advance makes static
partitioning unsuitable to shared clusters with frequently changing workloads.

