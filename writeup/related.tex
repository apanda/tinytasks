\section{Related Work}

There has been a large body of work in the context of distributed operating 
systems~\cite{douglis1991transparent, milojivcic2000process, amoeba-os, chorus} 
and virtual machines~\cite{xen, something} that have looked at scheduling and
context-switching among processes across different machines. Migration of
processes or virtual machines typically use premptive methods and we compare
them to our design goals in Section~\ref{sec:alternate}.

Recent work proposed Amoeba~\cite{ananthanarayanan2012true}, a system that
shares similar goals as our work. In Amoeba, the system identifies safe-points
where a task can be killed, while minimizing wasted work. At safe-points a task
can checkpoint output based on inputs it has processed so far, and a new task is
spawned to process the remaining inputs. This work is perhaps closest to
TinyTasks, and shows that preemptibility is an important feature for tasks
scheduled on a cluster.  However, while the work acknowledges the necessity for
preemptibility, it places no requirements on how frequently safe-point occur,
and hence places no bounds on the maximum time for which a single task can
capture the processor.  Furthermore, actually determining safe-points for
general tasks is non-trivial, and while Amoeba provides some examples of
safe-points for certain kinds of tasks, they do not provide a generic framework
for determining where such safe-points could occur.  TinyTasks by contrast do
not require any program analysis to identify such points, and instead relies on
limiting input sizes to achieve preemption.

Dremel~\cite{melnik2010dremel}, in conjunction with Google's cluster scheduler
Omega~\cite{wilkes-berkeley}, has also been used to run interactive jobs in a
cluster otherwise used for batch processing. Dremel achieves low latency by
spinning up long-running agents on which queries are executed. This is similar
to statically partitioning a cluster, where a part of the cluster is reserved
for interactive jobs, and the rest can be used for batch jobs. Such a method
necessarily limits the utilization of a cluster, and requires that the peak load
for interactive jobs be known in advance, so as to ensure that constraints are
met. Requiring application requirements be known in advance makes static
partitioning unsuitable to shared clusters with frequently changing workloads.

\eat{\paragraph{Process Migration} Include operating systems work on process
migration, as well as Ganesh's Amoeba paper. This work is most similar to
our approach. With tiny tasks, we are effectively architecting for pre-emption,
without needing to worry about moving large amounts of contexts between a
stopped and re-started job.

\paragraph{Skew Mitigation} Large body of over-engineered work on skew-
mitigation; most solutions fix only part of the problem and are highly
complex

\paragraph{Running Batch Workloads Alongside Interactive Workloads}
Google touts the fact that Omega/Borg schedule for both batch and interactive
workloads, but what they do is effecitively static partitioning (interactive
services have some portion of the cluster that they re-use to execute requests,
rather than scheduling a new job for each user request)! E.g., 
Dremel has a statically allocated part of the cluster.}

\eat{
The benefits of preemption and short task length are well studied in operating
systems, for instance~\cite{sherman1972trace} shows that operating system
schedulers should ideally be preemptive, and should prevent jobs from capturing
the CPU for too long a time. This is unfortunately harder to achieve in the
cluster computing context, since resource allocation includes much more than the
CPU, and revoking allocated resources might require moving the task. Previous
work has also looked into process
migration~\cite{douglis1991transparent,milojivcic2000process}, where processes
are transparently moved between machines. These methods present overheads, as
process context and state needs to be transfered across the network. For large
tasks such state can be of significant size, and such transfer would put a
significant load on the data center network. 
}
